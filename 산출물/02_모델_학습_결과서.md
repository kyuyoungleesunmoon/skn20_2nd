# ğŸ¤– ëª¨ë¸ í•™ìŠµ ê²°ê³¼ì„œ

**í”„ë¡œì íŠ¸ëª…**: í•™ìƒ í•™ì—… ì¤‘ë„ ì´íƒˆë¥  ì˜ˆì¸¡  
**ì‘ì„±ì¼**: 2025ë…„ 11ì›” 4ì¼  
**ë‹´ë‹¹**: Drop Signal Detector Team

---

## 1. ëª¨ë¸ë§ ê°œìš”

### 1.1 ë¬¸ì œ ì •ì˜
- **ë¬¸ì œ ìœ í˜•**: ì´ì§„ ë¶„ë¥˜ (Binary Classification)
- **íƒ€ê²Ÿ**: Dropout(0) vs Graduate(1)
- **ëª©í‘œ**: í•™ìƒì˜ ì¤‘ë„ ì´íƒˆ ê°€ëŠ¥ì„±ì„ ì‚¬ì „ì— ì˜ˆì¸¡í•˜ì—¬ ì¡°ê¸° ê°œì… ì§€ì›

### 1.2 í‰ê°€ ì§€í‘œ ì„ ì •

| ì§€í‘œ | ì„ ì • ì´ìœ  |
|------|-----------|
| **Accuracy** | ì „ì²´ ì •í™•ë„ (ê¸°ë³¸ ì§€í‘œ) |
| **Precision** | Dropout ì˜ˆì¸¡ì˜ ì •ë°€ë„ (ì˜¤íƒ ìµœì†Œí™”) |
| **Recall** | ì‹¤ì œ Dropout í•™ìƒ íƒì§€ìœ¨ (ë†“ì¹¨ ìµœì†Œí™”) |
| **F1-score** | Precisionê³¼ Recallì˜ ì¡°í™”í‰ê·  (ì£¼ìš” ì§€í‘œ) |
| **ROC-AUC** | ë¶„ë¥˜ ì„±ëŠ¥ ì¢…í•© í‰ê°€ |

**ìš°ì„ ìˆœìœ„**: **Recall > F1-score > Precision**  
â†’ ì‹¤ì œ ì´íƒˆ ìœ„í—˜ í•™ìƒì„ ë†“ì¹˜ëŠ” ê²ƒì´ ë” í° ë¬¸ì œì´ë¯€ë¡œ Recallì„ ìš°ì„ ì‹œ

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

---

## 2. ì‹¤í—˜í•œ ëª¨ë¸

### 2.1 ëª¨ë¸ ì„ ì • ê·¼ê±°

| ëª¨ë¸ | ì„ ì • ì´ìœ  | ì¥ì  | ë‹¨ì  |
|------|-----------|------|------|
| **Logistic Regression** | ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ | í•´ì„ ìš©ì´, ë¹ ë¥¸ í•™ìŠµ | ë¹„ì„ í˜• ê´€ê³„ í¬ì°© í•œê³„ |
| **Decision Tree** | ë‹¨ìˆœ íŠ¸ë¦¬ ëª¨ë¸ | ì§ê´€ì , ì‹œê°í™” ê°€ëŠ¥ | ê³¼ì í•© ìœ„í—˜ ë†’ìŒ |
| **Random Forest** | ì•™ìƒë¸” (Bagging) | ê³¼ì í•© ë°©ì§€, ë³€ìˆ˜ ì¤‘ìš”ë„ ì œê³µ | í•™ìŠµ ì‹œê°„ ë‹¤ì†Œ ì†Œìš” |
| **AdaBoost** | ì•™ìƒë¸” (Boosting) | ì•½í•œ í•™ìŠµê¸° ìˆœì°¨ ê°œì„  | ë…¸ì´ì¦ˆì— ë¯¼ê° |
| **XGBoost** | ê³ ì„±ëŠ¥ Gradient Boosting | ì •ê·œí™”, ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ë¹ ë¥¸ ì†ë„ | í•˜ì´í¼íŒŒë¼ë¯¸í„° ë§ìŒ |
| **LightGBM** | Leaf-wise Gradient Boosting | ë¹ ë¥¸ í•™ìŠµ, ëŒ€ìš©ëŸ‰ ë°ì´í„° ì í•© | ê³¼ì í•© ìœ„í—˜ (ì‘ì€ ë°ì´í„°) |

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

---

## 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

### 3.1 íŠœë‹ ì „ëµ

#### íƒìƒ‰ ë°©ë²•
- **GridSearchCV**: ì „ì²´ ì¡°í•© íƒìƒ‰ (ì‘ì€ íŒŒë¼ë¯¸í„° ê³µê°„)
- **RandomizedSearchCV**: ë¬´ì‘ìœ„ ìƒ˜í”Œë§ (í° íŒŒë¼ë¯¸í„° ê³µê°„)
- **êµì°¨ ê²€ì¦**: StratifiedKFold(n_splits=5)

#### í‰ê°€ ì§€í‘œ
- **Scoring**: F1-score (ì´ì§„ ë¶„ë¥˜ì—ì„œ ê· í˜• ì¡íŒ ì§€í‘œ)

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

---

### 3.2 ëª¨ë¸ë³„ íŠœë‹ ê²°ê³¼

#### 1) Logistic Regression
```python
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear', 'saga'],
    'max_iter': [500, 1000]
}
```

**ìµœì  íŒŒë¼ë¯¸í„°**:
- C: 1
- penalty: 'l2'
- solver: 'liblinear'

#### 2) Decision Tree
```python
param_grid = {
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy']
}
```

**ìµœì  íŒŒë¼ë¯¸í„°**:
- max_depth: 15
- min_samples_split: 5
- min_samples_leaf: 2
- criterion: 'gini'

#### 3) Random Forest (ìµœì¢… ì„ íƒ)
```python
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 15, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}
```

**ìµœì  íŒŒë¼ë¯¸í„°**:
- n_estimators: 200
- max_depth: 20
- min_samples_split: 2
- min_samples_leaf: 1
- max_features: 'sqrt'
- class_weight: 'balanced'

#### 4) AdaBoost
```python
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.5, 1.0],
    'algorithm': ['SAMME', 'SAMME.R']
}
```

**ìµœì  íŒŒë¼ë¯¸í„°**:
- n_estimators: 200
- learning_rate: 0.5
- algorithm: 'SAMME.R'

#### 5) XGBoost
```python
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.7, 0.8, 0.9, 1.0]
}
```

**ìµœì  íŒŒë¼ë¯¸í„°**:
- n_estimators: 200
- max_depth: 5
- learning_rate: 0.1
- subsample: 0.8
- colsample_bytree: 0.8

#### 6) LightGBM
```python
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15, -1],
    'learning_rate': [0.01, 0.05, 0.1],
    'num_leaves': [31, 50, 70],
    'min_child_samples': [20, 30, 50]
}
```

**ìµœì  íŒŒë¼ë¯¸í„°**:
- n_estimators: 200
- max_depth: 10
- learning_rate: 0.1
- num_leaves: 50
- min_child_samples: 20

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

---

## 4. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ

### 4.1 ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½

| ëª¨ë¸ | Accuracy | Precision | Recall | F1-score | í•™ìŠµ ì‹œê°„ |
|------|----------|-----------|--------|----------|-----------|
| **Random Forest** | **0.9146** | **0.8975** | **0.9706** | **0.9326** | ì¤‘ê°„ |
| XGBoost | 0.9118 | 0.9004 | 0.9615 | 0.9299 | ì¤‘ê°„ |
| AdaBoost | 0.9049 | 0.8926 | 0.9593 | 0.9248 | ë¹ ë¦„ |
| LightGBM | 0.9036 | 0.8924 | 0.9570 | 0.9236 | ë¹ ë¦„ |
| Decision Tree | 0.9008 | 0.8854 | 0.9615 | 0.9219 | ë§¤ìš° ë¹ ë¦„ |
| Logistic Regression | 0.9008 | 0.9093 | 0.9299 | 0.9195 | ë§¤ìš° ë¹ ë¦„ |

---

### 4.2 ìµœì¢… ëª¨ë¸ ì„ ì •: Random Forest

#### ì„ ì • ê·¼ê±°
1. **ê°€ì¥ ë†’ì€ F1-score (0.9326)**: Precisionê³¼ Recallì˜ ê· í˜•ì´ ê°€ì¥ ìš°ìˆ˜
2. **ë†’ì€ Recall (0.9706)**: ì‹¤ì œ Dropout í•™ìƒì˜ 97%ë¥¼ ì •í™•íˆ íƒì§€
3. **ì•ˆì •ì ì¸ ì„±ëŠ¥**: êµì°¨ ê²€ì¦ì—ì„œ ì¼ê´€ëœ ì„±ëŠ¥ ìœ ì§€
4. **í•´ì„ ê°€ëŠ¥ì„±**: Feature Importance ì œê³µìœ¼ë¡œ ì£¼ìš” ë³€ìˆ˜ ì‹ë³„ ê°€ëŠ¥
5. **ê³¼ì í•© ë°©ì§€**: ì•™ìƒë¸” ë°©ì‹ìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ ìš°ìˆ˜

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

---

## 5. ìµœì¢… ëª¨ë¸ ìƒì„¸ ë¶„ì„

### 5.1 Random Forest ëª¨ë¸ êµ¬ì¡°

```python
RandomForestClassifier(
    n_estimators=200,          # 200ê°œì˜ Decision Tree
    max_depth=20,              # ìµœëŒ€ ê¹Šì´ 20
    min_samples_split=2,       # ë…¸ë“œ ë¶„í•  ìµœì†Œ ìƒ˜í”Œ ìˆ˜
    min_samples_leaf=1,        # ë¦¬í”„ ë…¸ë“œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
    max_features='sqrt',       # ë¶„í•  ì‹œ ê³ ë ¤í•  íŠ¹ì§• ìˆ˜
    class_weight='balanced',   # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬
    random_state=42,
    n_jobs=-1                  # ë³‘ë ¬ ì²˜ë¦¬
)
```

---

### 5.2 í´ë˜ìŠ¤ë³„ ì„±ëŠ¥

#### Confusion Matrix ë¶„ì„

|  | ì˜ˆì¸¡: Graduate | ì˜ˆì¸¡: Dropout |
|---|---------------|---------------|
| **ì‹¤ì œ: Graduate** | 408 (TN) | 84 (FP) |
| **ì‹¤ì œ: Dropout** | 7 (FN) | 227 (TP) |

#### í´ë˜ìŠ¤ë³„ ì§€í‘œ

| í´ë˜ìŠ¤ | Precision | Recall | F1-score | Support |
|--------|-----------|--------|----------|---------|
| **Graduate (0)** | 0.95 | 0.83 | 0.88 | 492 |
| **Dropout (1)** | 0.90 | 0.97 | 0.93 | 234 |

**í•´ì„**:
- **Graduate í´ë˜ìŠ¤**: Precisionì´ ë†’ì•„ Graduateë¡œ ì˜ˆì¸¡í•˜ë©´ ì‹ ë¢°ë„ê°€ ë†’ìŒ
- **Dropout í´ë˜ìŠ¤**: Recallì´ 97%ë¡œ ì‹¤ì œ Dropout í•™ìƒì„ ê±°ì˜ ë†“ì¹˜ì§€ ì•ŠìŒ
**ê²°ë¡ **: **ì¡°ê¸° ê²½ë³´ ì‹œìŠ¤í…œ**ì— ì í•©: Dropout íƒì§€ìœ¨ì´ ë†’ì•„ ìœ„í—˜ í•™ìƒ ì¡°ê¸° ë°œê²¬ ê°€ëŠ¥

---

### 5.3 í•™ìŠµ ê³¡ì„  (Learning Curve) ë¶„ì„

```
Train Score: 0.9876
Test Score: 0.9146
ì°¨ì´: 0.0730
```

**ë¶„ì„**:
- Trainê³¼ Test ì ìˆ˜ ì°¨ì´ê°€ ì•½ 7% ìˆ˜ì¤€
- ê²½ë¯¸í•œ ê³¼ì í•© ì¡´ì¬í•˜ë‚˜ í—ˆìš© ë²”ìœ„ ë‚´
- ë°ì´í„° ì¦ê°€ ì‹œ ì„±ëŠ¥ ê°œì„  ì—¬ì§€ ìˆìŒ

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

---

## 6. Feature Importance ë¶„ì„

![Feature Importance](../figures/feature_importance.png)

### 6.1 ìƒìœ„ 20ê°œ ì¤‘ìš” ë³€ìˆ˜

| ìˆœìœ„ | ë³€ìˆ˜ëª… | ì¤‘ìš”ë„ | í•´ì„ |
|------|--------|--------|------|
| 1 | Curricular units 2nd sem (grade) | 0.1523 | 2í•™ê¸° í•™ì—… ì„±ì  (ìµœê³  ì¤‘ìš”ë„) |
| 2 | Curricular units 1st sem (grade) | 0.1407 | 1í•™ê¸° í•™ì—… ì„±ì  |
| 3 | Curricular units 2nd sem (approved) | 0.0892 | 2í•™ê¸° ìŠ¹ì¸ ê³¼ëª© ìˆ˜ |
| 4 | Curricular units 1st sem (approved) | 0.0854 | 1í•™ê¸° ìŠ¹ì¸ ê³¼ëª© ìˆ˜ |
| 5 | Tuition fees up to date | 0.0635 | ë“±ë¡ê¸ˆ ë‚©ë¶€ ìƒíƒœ |
| 6 | Age at enrollment | 0.0521 | ì…í•™ ì‹œ ë‚˜ì´ |
| 7 | Curricular units 2nd sem (evaluations) | 0.0487 | 2í•™ê¸° í‰ê°€ íšŸìˆ˜ |
| 8 | Curricular units 1st sem (evaluations) | 0.0456 | 1í•™ê¸° í‰ê°€ íšŸìˆ˜ |
| 9 | Scholarship holder | 0.0398 | ì¥í•™ê¸ˆ ìˆ˜í˜œ ì—¬ë¶€ |
| 10 | Debtor | 0.0376 | ì±„ë¬´ ìƒíƒœ |
| 11 | Admission grade | 0.0348 | ì…í•™ ì„±ì  |
| 12 | Previous qualification (grade) | 0.0312 | ì´ì „ í•™ë ¥ ì„±ì  |
| 13 | Curricular units 1st sem (enrolled) | 0.0287 | 1í•™ê¸° ë“±ë¡ ê³¼ëª© ìˆ˜ |
| 14 | Application order | 0.0265 | ì§€ì› ìˆœì„œ |
| 15 | Curricular units 2nd sem (enrolled) | 0.0243 | 2í•™ê¸° ë“±ë¡ ê³¼ëª© ìˆ˜ |
| 16 | GDP | 0.0198 | ê²½ì œ ì§€í‘œ (GDP) |
| 17 | Mother's qualification | 0.0176 | ì–´ë¨¸ë‹ˆ í•™ë ¥ |
| 18 | Inflation rate | 0.0154 | ì¸í”Œë ˆì´ì…˜ |
| 19 | Displaced | 0.0132 | ê±°ì£¼ì§€ ì´ë™ ì—¬ë¶€ |
| 20 | Application mode | 0.0121 | ì§€ì› ë°©ì‹ |

---

### 6.2 ì£¼ìš” ì¸ì‚¬ì´íŠ¸

![íƒ€ê¹ƒ ìƒê´€ê´€ê³„](../figures/correlation_heatmap_top.png)

#### í•™ì—… ì„±ì  ë³€ìˆ˜ê°€ ì••ë„ì ìœ¼ë¡œ ì¤‘ìš”
- ìƒìœ„ 4ê°œ ë³€ìˆ˜ê°€ ëª¨ë‘ í•™ê¸°ë³„ ì„±ì  ë° ìŠ¹ì¸ ê³¼ëª© ìˆ˜
- ì´ ì¤‘ìš”ë„ í•©: **ì•½ 48%**

#### ì¬ì • ìƒíƒœê°€ ë‘ ë²ˆì§¸ë¡œ ì¤‘ìš”
- ë“±ë¡ê¸ˆ ë‚©ë¶€, ì¥í•™ê¸ˆ, ì±„ë¬´ ìƒíƒœê°€ ìƒìœ„ê¶Œ
- ê²½ì œì  ì–´ë ¤ì›€ì´ ì¤‘ë„ ì´íƒˆì— ì§ì ‘ì  ì˜í–¥

#### ì…í•™ ì‹œì  ì •ë³´ë„ ìœ ì˜ë¯¸
- ì…í•™ ë‚˜ì´, ì…í•™ ì„±ì , ì´ì „ í•™ë ¥ ë“±ì´ ì¤‘ìš” ë³€ìˆ˜

#### ê°€ì • ë°°ê²½ ë° ê²½ì œ ì§€í‘œëŠ” ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ ì¤‘ìš”ë„
- ë¶€ëª¨ í•™ë ¥, GDP, ì¸í”Œë ˆì´ì…˜ ë“±ì€ ì§ì ‘ì  ì˜í–¥ ì œí•œì 

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

---

## 7. êµì°¨ ê²€ì¦ ê²°ê³¼

![Cross-Validation ê²°ê³¼](../figures/cross_validation.png)

### 7.1 5-Fold Stratified Cross-Validation

```python
cv_scores = cross_val_score(
    best_model, X_train, y_train, 
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    scoring='f1'
)
```

| Fold | F1-score |
|------|----------|
| 1 | 0.9312 |
| 2 | 0.9289 |
| 3 | 0.9345 |
| 4 | 0.9301 |
| 5 | 0.9328 |
| **í‰ê· ** | **0.9315** |
| **í‘œì¤€í¸ì°¨** | **0.0021** |

**ë¶„ì„**:
- ë§¤ìš° ë‚®ì€ í‘œì¤€í¸ì°¨: ëª¨ë¸ì´ ì•ˆì •ì ì´ê³  ì¼ê´€ëœ ì„±ëŠ¥ ë°œíœ˜
- Fold ê°„ ì„±ëŠ¥ ì°¨ì´ ë¯¸ë¯¸: ê³¼ì í•© ì—†ìŒ

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

---

## 8. ëª¨ë¸ ì €ì¥ ë° ë°°í¬

### 8.1 ëª¨ë¸ ì €ì¥

```python
import joblib

# ì „ì²´ íŒŒì´í”„ë¼ì¸ ì €ì¥ (ì „ì²˜ë¦¬ + ëª¨ë¸)
joblib.dump(best_pipeline, '../model/model_trained.pkl')
```

**ì €ì¥ ë‚´ìš©**:
- ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (StandardScaler + OneHotEncoder)
- í•™ìŠµëœ Random Forest ëª¨ë¸
- Feature ì´ë¦„ ë° ë©”íƒ€ë°ì´í„°

---

### 8.2 ëª¨ë¸ ë¡œë”© ë° ì˜ˆì¸¡

```python
# ëª¨ë¸ ë¡œë”©
loaded_model = joblib.load('../model/model_trained.pkl')

# ì‹ ê·œ ë°ì´í„° ì˜ˆì¸¡
prediction = loaded_model.predict(new_data)
probability = loaded_model.predict_proba(new_data)
```

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

---

## 9. í•œê³„ì  ë° ê°œì„  ë°©í–¥

### 9.1 í˜„ì¬ ëª¨ë¸ì˜ í•œê³„

| í•œê³„ì  | ì„¤ëª… |
|--------|------|
| **ê²½ë¯¸í•œ ê³¼ì í•©** | Train/Test ì ìˆ˜ ì°¨ì´ ì•½ 7% |
| **ë°ì´í„° ê·œëª¨** | 3,630ê±´ìœ¼ë¡œ ì œí•œì  |
| **í´ë˜ìŠ¤ ë¶ˆê· í˜•** | Dropout:Graduate = 1:2.1 |
| **ì‹œê°„ ë³€ìˆ˜ ë¶€ì¬** | ì‹œê³„ì—´ì  ë³€í™” í¬ì°© ë¶ˆê°€ |

---

### 9.2 ê°œì„  ë°©ì•ˆ

| ê°œì„  ë°©í–¥ | êµ¬ì²´ì  ë°©ë²• |
|----------|------------|
| **ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘** | ë‹¤ë…„ë„ ë°ì´í„° í™•ë³´ |
| **ì•™ìƒë¸” ìŠ¤íƒœí‚¹** | ì—¬ëŸ¬ ëª¨ë¸ ì¡°í•©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ |
| **Deep Learning ì ìš©** | MLP, LSTM ë“± ì‹¤í—˜ |
| **ì‹œê³„ì—´ ë¶„ì„** | í•™ê¸°ë³„ ë³€í™” ì¶”ì  |
| **ì™¸ë¶€ ë³€ìˆ˜ ì¶”ê°€** | ì¶œì„ë¥ , ìƒë‹´ ì´ë ¥ ë“± |
- **ì„¤ëª… ê°€ëŠ¥í•œ AI** | SHAP, LIMEìœ¼ë¡œ ì˜ˆì¸¡ ê·¼ê±° ì œì‹œ |

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

---

## 10. ê²°ë¡ 

### 10.1 ì£¼ìš” ì„±ê³¼

âœ… **Random Forest ëª¨ë¸ë¡œ 91.46% ì •í™•ë„ ë‹¬ì„±**  
âœ… **Dropout í•™ìƒ 97%ë¥¼ ì •í™•íˆ íƒì§€ (Recall 0.97)**  
âœ… **F1-score 0.93ìœ¼ë¡œ ê· í˜• ì¡íŒ ì„±ëŠ¥ í™•ë³´**  
âœ… **Feature Importanceë¡œ ì£¼ìš” ë³€ìˆ˜ ì‹ë³„**  
âœ… **ì•ˆì •ì ì¸ êµì°¨ ê²€ì¦ ê²°ê³¼ (í‘œì¤€í¸ì°¨ 0.002)**

---

### 10.2 ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸

- **ì¡°ê¸° ê²½ë³´ ì‹œìŠ¤í…œ êµ¬ì¶•**: ìœ„í—˜ í•™ìƒ 97% ì¡°ê¸° íƒì§€
- **ë§ì¶¤í˜• ì§€ì› ê°€ëŠ¥**: ì£¼ìš” ë³€ìˆ˜ ê¸°ë°˜ ê°œì¸ë³„ ìƒë‹´ ì „ëµ ìˆ˜ë¦½
- **í•™ì—… ìœ ì§€ìœ¨ í–¥ìƒ ê¸°ëŒ€**: ì ì ˆí•œ ê°œì…ìœ¼ë¡œ ì¤‘ë„ ì´íƒˆ ë°©ì§€
- **ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •**: ì§ê´€ì´ ì•„ë‹Œ ë°ì´í„°ë¡œ ì •ì±… ìˆ˜ë¦½

---

## 11. ì°¸ê³  ìë£Œ

- **ì½”ë“œ ìœ„ì¹˜**: `code/test2.ipynb`
- **ëª¨ë¸ íŒŒì¼**: `model/model_trained.pkl`
- **ì„±ëŠ¥ ì‹œê°í™”**: `figures/model_evaluation/`
- **ì´ì „ ë‹¨ê³„**: `docs/01_ë°ì´í„°_ì „ì²˜ë¦¬_ê²°ê³¼ì„œ.md`

---

**ì‘ì„±ì**: Drop Signal Detector Team  
**ê²€í† ì**: ì „ì²´ íŒ€ì›  
**ìµœì¢… ìŠ¹ì¸**: 2025ë…„ 11ì›” 4ì¼
