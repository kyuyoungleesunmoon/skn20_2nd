# Comprehensive Technical Review - All Teams

> **Review Date**: December 3, 2025  
> **Teams Reviewed**: Team 1, Team 2, Team 3, Team 4, Team 5 (5 teams total)  
> **Review Aspects**: Code quality, Library currency, Structural design, Security, Performance, Documentation

---

## [Team 1] Bank Customer Churn Prediction Project

### ‚úÖ Strengths

This team demonstrated strong practical applicability. The Streamlit dashboard is impressively detailed with 1,289 lines of code, featuring an intuitive user interface. The modularization of IQR-based outlier detection and rare category handling functions shows good reusability practices.

From a modeling perspective, they compared 8 different algorithms (Logistic Regression, RandomForest, KNN, SVC, DecisionTree, Bagging, NN, AdaBoost) and pragmatically addressed class imbalance using the `class_weight='balanced'` strategy. The systematic validation of overfitting through Train/Test performance comparison is solid.

The README documentation is well-written, clearly showing the entire project flow from team introduction to EDA visualizations, model performance comparison, and final insights.

### ‚ùå Issues

**Missing Library Version Specifications**  
The `requirements.txt` file only lists library names without versions. All packages like `numpy`, `pandas`, `scikit-learn`, `matplotlib` are recorded without versions, which could cause compatibility issues when trying to reproduce the environment later. Especially with scikit-learn, API changes between versions mean not fixing versions could break the code.

**Absence of Data Preprocessing Pipeline**  
The Streamlit app uses `LabelEncoder` and `StandardScaler`, but it's unclear if consistency is guaranteed between training and inference. Multiple `fit_transform` calls are made in different places, and it appears to fit anew each time rather than using saved scaler/encoder objects from training. This doesn't guarantee consistent transformation for test data.

**Hardcoded Paths and Filenames**  
File paths like `'Bank Customer Churn Prediction.csv'` are hardcoded, and relative path handling is unclear. In production environments, file locations can vary, but Path objects aren't properly utilized.

**Insufficient Exception Handling**  
While the file loading function has `try-except`, it only outputs an error message via `st.error` after catching exceptions and returns `None`. Subsequent code using the DataFrame without None checks could crash immediately.

**Absence of Test Code**  
While model performance validation exists, there are no unit tests for preprocessing functions or outlier detection logic. This creates risk of regression bugs during refactoring or maintenance.

### üîß Improvement Directions

**1. Fix Library Versions**  
Execute `pip freeze > requirements.txt` in your current environment to fix exact versions. At minimum, specify major versions:
```
numpy==1.24.3
pandas==2.0.3
scikit-learn==1.3.2
matplotlib==3.7.2
seaborn==0.12.2
streamlit==1.28.0
```

**2. Introduce sklearn Pipeline**  
Bundle preprocessing and model into a single Pipeline to ensure consistency between training and inference:
```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

preprocessor = ColumnTransformer([
    ('num', StandardScaler(), numerical_cols),
    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
])

model_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(class_weight='balanced'))
])
```
This way, both preprocessing and model are saved in a single `model_pipeline.pkl` file, ensuring reproducibility.

**3. Separate Configuration Files**  
Separate file paths, feature names, hyperparameters into YAML or JSON configuration files:
```python
# config.yaml
data:
  raw_path: "data/raw/Bank_Customer_Churn_Prediction.csv"
  processed_path: "data/processed/"
  
features:
  numerical: ["credit_score", "age", "balance", ...]
  categorical: ["gender", "country"]
```

**4. Robust Exception Handling**  
```python
def load_data(filepath):
    try:
        if not Path(filepath).exists():
            raise FileNotFoundError(f"File not found: {filepath}")
        df = pd.read_csv(filepath)
        if df.empty:
            raise ValueError("Data is empty")
        return df
    except Exception as e:
        st.error(f"Data loading failed: {str(e)}")
        st.stop()  # Stop app execution
```

**5. Add Simple Tests**  
Add pytest to test core logic:
```python
def test_outlier_detection():
    test_df = pd.DataFrame({'age': [20, 25, 30, 100, 35]})
    outliers, lower, upper = detect_outliers_iqr(test_df, 'age')
    assert len(outliers) == 1
    assert outliers['age'].values[0] == 100
```

---

## [Team 2] Gym Member Churn Prediction Model

### ‚úÖ Strengths

This team demonstrated the most systematic project structure among the 5 teams. Notebooks are clearly separated into EDA, Model Training, and Model Evaluation, with deliverables well-organized in markdown and PDF formats. Particularly impressive is the step-by-step documentation tracking F1 Score improvement from 0.7373 to 0.9657.

Technically, they effectively utilized advanced techniques including SMOTE for class imbalance, 11 derived features (Lifetime_per_Month, Is_New_Member, Class_Engagement, etc.), hyperparameter tuning via RandomizedSearchCV, and Ultimate Stacking Ensemble. Achieving 30%+ performance improvement by tuning XGBoost and LightGBM 50 times each is excellent.

The README is extremely detailed, with project structure, tech stack, installation methods, execution guides, and business insights all at a production-ready level of completeness.

### ‚ùå Issues

**Absence of Library Dependency Management**  
Ironically, despite such high project quality, there's no `requirements.txt` file at all. The README only lists "Required Packages" as text without version information. While mentioning minimum versions like `tensorflow >= 2.13.0`, more specific version pinning is needed in practice.

**Hardcoded Model File Paths**  
Paths like `'project/models/2024_churn_model/stacking_ultimate.pkl'` are hardcoded. These absolute paths become problematic when running in different environments, especially when deploying to Streamlit where path structures may differ.

**Large Model File Management**  
Model files like `nn_model.h5`, `stacking_ultimate.pkl` may be included in Git (needs verification). Model files are large and should use Git LFS or separate storage.

**Potential Use of Deprecated TensorFlow APIs**  
While claiming to use TensorFlow 2.13+, it's unclear if the `model.fit()` usage with parameters like `validation_split` aligns with latest recommendations. TensorFlow 2.x recommends using `tf.keras.callbacks` and `tf.data` API.

**Performance Optimization Opportunities**  
The Stacking Ensemble uses 4 base models, requiring all 4 to be loaded and executed during inference, which is inefficient in terms of memory and response time. In actual production, a single optimal model or lightweight ensemble might be better.

### üîß Improvement Directions

**1. Create requirements.txt with Fixed Versions**  
Extract and fix exact versions from current environment:
```
pandas==2.0.3
numpy==1.24.3
scikit-learn==1.3.2
xgboost==2.0.3
lightgbm==4.1.0
tensorflow==2.15.0
imbalanced-learn==0.11.0
matplotlib==3.8.2
seaborn==0.13.0
streamlit==1.29.0
```

**2. Environment Variables and Path Management**  
Manage paths using `.env` files and `python-dotenv`:
```python
# .env
MODEL_DIR=./models/2024_churn_model
DATA_DIR=./data

# In code
from dotenv import load_dotenv
import os
load_dotenv()
model_path = os.getenv('MODEL_DIR') + '/stacking_ultimate.pkl'
```

**3. Git LFS Configuration**  
Manage large model files with Git LFS:
```bash
git lfs install
git lfs track "*.pkl"
git lfs track "*.h5"
git add .gitattributes
```

**4. Utilize Latest TensorFlow APIs**  
Write callbacks and data pipelines in modern style:
```python
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

model.fit(
    train_dataset,  # Use tf.data.Dataset
    validation_data=val_dataset,
    epochs=100,
    callbacks=[early_stopping]
)
```

**5. Model Serving Optimization**  
Consider lightweight approaches for real-time inference:
- Use only top 1-2 models from ensemble
- Convert to ONNX for faster inference
- Apply model quantization
```python
import onnxruntime as ort
# Convert sklearn model to ONNX
from skl2onnx import convert_sklearn
onnx_model = convert_sklearn(model, ...)
```

**6. Add Logging**  
Logging is essential in production:
```python
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info(f"Model loaded from {model_path}")
logger.warning(f"Low confidence prediction: {proba}")
```

---

## [Team 3] Internet Customer Churn Analysis

### ‚úÖ Strengths

This team conducted extensive deep learning experiments. They explored three frameworks - PyTorch MLP, sklearn MLPClassifier, and TensorFlow models. Particularly impressive is implementing hyperparameter search with PyTorch in `torch_model_randomsearch.ipynb`, showing good initiative.

The separation of ML and DL directories to explore both traditional machine learning and deep learning approaches is good. Data preprocessing and EDA are organized in separate notebooks, with PDF deliverables well-prepared for each stage.

### ‚ùå Issues

**Absence of Python Script Files**  
All code exists only as Jupyter notebooks. In practice, notebooks need to be converted into modular Python scripts, but this step is completely missing. There's no deployment interface like Streamlit or FastAPI.

**No requirements.txt**  
The only team among 5 without any dependency management file. Using PyTorch, TensorFlow, and scikit-learn without knowing which versions is critically problematic for reproducibility.

**Code Duplication and Lack of Modularization**  
Similar preprocessing logic is repeated across multiple notebooks. Expecting duplication between `data_preprocessing_ml.ipynb` and `data_preprocessing_tree_EDA.ipynb`. Common functions weren't separated into utility modules.

**Inefficiency of PyTorch and TensorFlow Mixed Use**  
While using both frameworks is good, there's no clear reason why both are needed. A comparative analysis of each framework's advantages would have been valuable, but it appears to be mere experimentation. In practice, it's better to choose one and go deep.

**Unsystematic Hyperparameter Search**  
Appears to have implemented `randomsearch` manually, but using specialized libraries like Optuna or Ray Tune would be much more efficient. Unclear if best practices like Early Stopping and Learning Rate Scheduling were applied to PyTorch models.

**Insufficient Documentation**  
While an EDA report exists, model selection rationale and performance comparison results aren't clearly organized in README. It's difficult to know which model was finally selected and why.

### üîß Improvement Directions

**1. Code Modularization and Packaging**  
Separate notebook core logic into Python modules:
```
project/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cleaner.py
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ml_models.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dl_models.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ evaluation.py
‚îú‚îÄ‚îÄ notebooks/  # Use only for experimentation
‚îî‚îÄ‚îÄ app/  # Streamlit or API
```

**2. Specify Dependencies and Manage Environment**  
Pay attention to version compatibility when using both PyTorch and TensorFlow:
```
torch==2.1.0
torchvision==0.16.0
tensorflow==2.15.0
scikit-learn==1.3.2
numpy==1.24.3
pandas==2.0.3
optuna==3.4.0  # For hyperparameter tuning
```

**3. Unify Framework or Clear Separation**  
If using both PyTorch and TensorFlow, clarify each purpose:
- PyTorch: Research prototyping, custom structure experiments
- TensorFlow: Production deployment, utilizing TF Serving

Or unify to one for deep optimization.

**4. Introduce Optuna**  
Systematize hyperparameter tuning with Optuna:
```python
import optuna

def objective(trial):
    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)
    hidden_size = trial.suggest_int('hidden_size', 32, 512)
    
    model = MLP(hidden_size=hidden_size)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    
    # Train and validate
    val_loss = train_and_evaluate(model, optimizer)
    return val_loss

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100)
```

**5. Experiment Management with MLflow**  
Systematically track multiple experiments:
```python
import mlflow

with mlflow.start_run():
    mlflow.log_params({"lr": 0.001, "hidden_size": 128})
    mlflow.log_metric("val_loss", val_loss)
    mlflow.pytorch.log_model(model, "model")
```

**6. README Improvement**  
Clearly document final model selection rationale and performance comparison table:
```markdown
## Model Comparison Results

| Model | Framework | F1 Score | Training Time | Inference Time |
|-------|-----------|----------|---------------|----------------|
| Logistic Regression | sklearn | 0.82 | 1s | <1ms |
| Random Forest | sklearn | 0.85 | 30s | 5ms |
| MLP (sklearn) | sklearn | 0.83 | 15s | 2ms |
| MLP (PyTorch) | PyTorch | 0.87 | 120s | 3ms |
| CNN (TensorFlow) | TensorFlow | 0.84 | 180s | 4ms |

**Final Selection**: PyTorch MLP (F1=0.87, good inference speed)
```

**7. Add Deployment Interface**  
At minimum, create a Streamlit app:
```python
# app.py
import streamlit as st
import torch
from src.models.dl_models import MLP

model = MLP.load_from_checkpoint('best_model.pth')

st.title("Customer Churn Prediction")
features = st.form("input_form")
# ... get input
prediction = model.predict(features)
st.write(f"Churn Probability: {prediction:.2%}")
```

---

## [Team 4] Customer Churn Prediction (Insurance/Finance)

### ‚úÖ Strengths

This team demonstrates the highest code quality. Type Hints are properly used (`typing.Any, Dict, List, Optional`, etc.), and scikit-learn best practices like `ColumnTransformer` and `Pipeline` are accurately applied in Streamlit code. Particularly good is safely handling unknown categories with `OneHotEncoder(handle_unknown='ignore')`.

Requirements.txt clearly specifies versions. With major versions fixed like `pandas==2.0.3`, `scikit-learn==1.3.0`, `xgboost==1.7.6`, environment reproduction is easy. Ensuring model interpretability using SHAP is excellent.

Notebooks are well-separated by role: EDA, Model Training Pipeline, Model Evaluation. Deliverables for each stage are cleanly organized in markdown.

### ‚ùå Issues

**Somewhat Outdated Library Versions**  
`xgboost==1.7.6` is an early 2023 version. The latest is 2.0.3, missing performance improvements and new features by using 1.x. Same with `lightgbm==4.0.0` - latest is 4.1.0+.

`streamlit==1.25.0` is also quite old. Latest Streamlit (1.28+) has useful features like `st.fragment` and `st.dialog` that aren't being utilized.

**Unclear imbalanced-learn Usage**  
Requirements.txt includes `imbalanced-learn==0.11.0`, but need to verify if SMOTE or ADASYN was actually used. Including unused libraries is unnecessary capacity waste.

**Joblib Instead of Pickle Consideration**  
Using `joblib` for model saving is good, but since scikit-learn 1.3+, `sklearn.externals.joblib` is deprecated and using independent joblib is recommended. If already doing this, no problem, but code needs verification.

**Insufficient Error Handling**  
While using Pipeline is good, handling logic for exceptions that could occur during Pipeline execution (e.g., new category values, missing values) appears lacking. While `handle_unknown='ignore'` exists, other edge case handling isn't clear.

**Absence of Performance Monitoring**  
Using SHAP for interpretation, but no monitoring of real-time model performance (inference speed, memory usage). These metrics are important in production environments.

### üîß Improvement Directions

**1. Update Library Versions**  
Upgrade to latest stable versions:
```
pandas==2.1.4
numpy==1.26.2
scikit-learn==1.3.2
xgboost==2.0.3  # Major upgrade
lightgbm==4.1.0
matplotlib==3.8.2
seaborn==0.13.0
streamlit==1.29.0  # New features available
plotly==5.18.0
shap==0.43.0
```

Run tests after upgrade to verify no API changes.

**2. Dependency Audit**  
Remove actually unused libraries:
```python
# Find unused imports
pip install pipreqs
pipreqs . --force  # Extract only actually used packages
```

**3. Robust Pipeline Error Handling**  
```python
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin

class SafeTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, transformer):
        self.transformer = transformer
    
    def fit(self, X, y=None):
        try:
            self.transformer.fit(X, y)
        except Exception as e:
            logger.error(f"Transformer fit failed: {e}")
            raise
        return self
    
    def transform(self, X):
        try:
            return self.transformer.transform(X)
        except Exception as e:
            logger.error(f"Transform failed: {e}")
            # Replace with default or re-raise exception
            raise
```

**4. Add Performance Profiling**  
Measure inference time and memory:
```python
import time
import psutil
import streamlit as st

start_time = time.time()
memory_before = psutil.virtual_memory().used / 1024**2

prediction = pipeline.predict(input_df)[0]

memory_after = psutil.virtual_memory().used / 1024**2
elapsed_time = time.time() - start_time

st.sidebar.metric("Inference Time", f"{elapsed_time*1000:.1f}ms")
st.sidebar.metric("Memory Usage", f"{memory_after - memory_before:.1f}MB")
```

**5. Utilize XGBoost 2.0 New Features**  
XGBoost 2.0 has new `device='cuda'` parameter and improved categorical feature support:
```python
from xgboost import XGBClassifier

model = XGBClassifier(
    device='cuda',  # Use GPU (if available)
    enable_categorical=True,  # Direct categorical support
    tree_method='hist',  # Fast histogram-based
)
```

**6. Utilize Latest Streamlit Features**  
Use Streamlit 1.28+ new features:
```python
# Fragment for partial re-execution
@st.fragment
def prediction_section():
    if st.button("Run Prediction"):
        result = model.predict(...)
        st.write(result)

# Dialog for modal popup
@st.dialog("Prediction Details")
def show_details():
    st.write("Detailed analysis...")
```

**7. Save Model Metadata**  
Save metadata with model for clear version control:
```python
import joblib
from datetime import datetime

model_metadata = {
    'model_version': '1.0.0',
    'trained_date': datetime.now().isoformat(),
    'sklearn_version': sklearn.__version__,
    'xgboost_version': xgboost.__version__,
    'feature_names': X_train.columns.tolist(),
    'metrics': {'f1': 0.89, 'auc': 0.92}
}

joblib.dump({
    'model': pipeline,
    'metadata': model_metadata
}, 'model_with_metadata.pkl')
```

---

## [Team 5] Netflix Customer Churn Prediction

### ‚úÖ Strengths

This team excels in project structuring and modularization. Separated `streamlit_netflix/utils/` into `model_utils.py`, `preprocessing_utils.py`, `styles.py` clearly distinguishing concerns. Properly implemented Streamlit multi-page app structure (utilizing `pages/` directory).

Requirements.txt specifies minimum versions while maintaining flexibility (using `>=`). Including Optuna in dependencies is good - shows intent to do hyperparameter tuning properly.

README is visually rich, from team introduction to tech stack, like marketing material. Netflix-themed UI styling is meticulous.

### ‚ùå Issues

**Risk of Minimum Version Specification**  
Specifying only minimum versions like `numpy>=1.21.0` is a double-edged sword. When automatically upgrading to latest version, breaking changes could break code. Especially with numpy 2.0 having many API changes, this isn't considered.

**Circular Reference Risk Between Modules**  
With multiple modules in utils directory, structure where they import each other could cause circular references. If `model_utils.py` imports `preprocessing_utils.py` and vice versa, problems arise.

**Simultaneous Use of AdaBoost and RandomForest**  
Appears `model_utils.py` loads both `adaboost_model` and `rf_proba_model` simultaneously, but why use both models isn't clear. Seems one is for classification and one for probability prediction, which could confuse users.

**Streamlit Log File**  
`streamlit.log` file is included in repository, but should be added to `.gitignore`. Log files are generated during execution and not version control targets.

**SHAP Not Used**  
SHAP is included in requirements.txt but unclear if actually used. If added for model interpretability, should be actively utilized in code.

**Absence of Tests and Type Checking**  
With utils modules created, unit tests and type hints should be added but aren't. Especially core logic like `model_utils.py` absolutely needs testing.

### üîß Improvement Directions

**1. Specify Version Range**  
Specify both minimum and maximum versions to ensure stability:
```
numpy>=1.24.0,<2.0.0  # Avoid numpy 2.0 breaking changes
pandas>=1.5.0,<3.0.0
scikit-learn>=1.3.0,<1.4.0
xgboost>=1.7.0,<2.0.0
```

Or use poetry for more sophisticated dependency management:
```toml
[tool.poetry.dependencies]
python = "^3.10"
numpy = "^1.24.0"
pandas = "^2.0.0"
```

**2. Supplement .gitignore**  
```gitignore
# Log files
*.log
streamlit.log

# Cache
__pycache__/
*.pyc
.pytest_cache/

# Environment
.env
.venv/

# Jupyter
.ipynb_checkpoints/

# Model files (if large)
*.pkl
*.h5
*.pth
```

**3. Improve Module Structure and Add Type Hints**  
```python
# preprocessing_utils.py
from typing import List, Dict, Any
import pandas as pd
import numpy as np

def preprocess_features(
    data: pd.DataFrame,
    categorical_cols: List[str],
    numerical_cols: List[str]
) -> np.ndarray:
    """Feature preprocessing
    
    Args:
        data: Input DataFrame
        categorical_cols: List of categorical columns
        numerical_cols: List of numerical columns
    
    Returns:
        Preprocessed numpy array
    """
    # Implementation...
    return processed_data
```

**4. Model Wrapper Class**  
If using multiple models, create unified interface:
```python
# model_utils.py
from abc import ABC, abstractmethod
from typing import Optional

class ChurnPredictor(ABC):
    @abstractmethod
    def predict(self, features: np.ndarray) -> int:
        pass
    
    @abstractmethod
    def predict_proba(self, features: np.ndarray) -> float:
        pass

class EnsemblePredictor(ChurnPredictor):
    def __init__(self, adaboost_path: str, rf_path: str):
        self.adaboost = joblib.load(adaboost_path)
        self.rf = joblib.load(rf_path)
    
    def predict(self, features):
        # Use AdaBoost
        return self.adaboost.predict(features)[0]
    
    def predict_proba(self, features):
        # Use RandomForest probability
        return self.rf.predict_proba(features)[0, 1]
```

**5. Add Unit Tests with pytest**  
```python
# tests/test_preprocessing.py
import pytest
import pandas as pd
from streamlit_netflix.utils.preprocessing_utils import preprocess_features

def test_preprocess_basic():
    df = pd.DataFrame({
        'age': [25, 30, 35],
        'region': ['A', 'B', 'A']
    })
    result = preprocess_features(df, ['region'], ['age'])
    assert result.shape[0] == 3

def test_preprocess_missing_values():
    df = pd.DataFrame({
        'age': [25, None, 35],
        'region': ['A', 'B', None]
    })
    with pytest.raises(ValueError):
        preprocess_features(df, ['region'], ['age'])
```

**6. Actively Utilize SHAP**  
Properly use SHAP for model interpretation:
```python
import shap

# In Streamlit app
if st.checkbox("View Prediction Explanation"):
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(input_features)
    
    fig = shap.waterfall_plot(
        shap.Explanation(
            values=shap_values[0],
            base_values=explainer.expected_value,
            data=input_features[0],
            feature_names=feature_names
        )
    )
    st.pyplot(fig)
```

**7. Improve Configuration Management**  
Create `config.py` to centralize hardcoded values:
```python
# config.py
from dataclasses import dataclass
from pathlib import Path

@dataclass
class ModelConfig:
    adaboost_path: Path = Path("models/adaboost_model.pkl")
    rf_path: Path = Path("models/rf_proba_model.pkl")
    features: list = None
    
    def __post_init__(self):
        self.features = [
            'age', 'subscription_type', 'watch_hours',
            'last_login_days', 'device', 'monthly_fee'
        ]

config = ModelConfig()
```

**8. Improve Logging**  
Properly configure Python logging:
```python
# logging_config.py
import logging
from pathlib import Path

def setup_logging(log_file: str = "app.log"):
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )

# Usage
from logging_config import setup_logging
setup_logging()
logger = logging.getLogger(__name__)
logger.info("App started")
```

---

## Overall Assessment and Recommendations

### Common Issues Across All Teams

1. **Lack of Awareness of Dependency Management Importance**  
   - 3 out of 5 teams don't properly fix versions
   - Need to emphasize that reproducible environment building is key to project success

2. **Absence of Test Code**  
   - All teams didn't write unit tests
   - Recommend introducing pytest with minimum 50% coverage goal

3. **Insufficient Error Handling**  
   - Even when using try-except, only logging without recovery logic
   - Need user-friendly error messages and fallback strategies

4. **Lack of Security Considerations**  
   - Risk of hardcoding API keys or sensitive information in code
   - Recommend using .env files and environment variables

5. **Absence of Performance Monitoring**  
   - Not measuring model inference time, memory usage, etc.
   - Profiling essential before production deployment

### Top Priority Improvement Tasks by Team

| Team | Top Priority | Reason |
|------|-------------|--------|
| Team 1 | Fix requirements.txt versions + Introduce Pipeline | Ensure reproducibility and consistency |
| Team 2 | Create requirements.txt + Improve path management | High-quality project but lacks deployment readiness |
| Team 3 | Code modularization + Write dependency file | Notebook ‚Üí production code transition |
| Team 4 | Upgrade library versions | Utilize latest features and performance improvements |
| Team 5 | Type hints + Add tests | Add robustness to good structure |

### Learning Recommendations

Advanced topics worth trying in each team's next project:

1. **MLOps Pipeline**  
   - Experiment tracking with MLflow or Weights & Biases
   - Data version control with DVC
   - CI/CD with GitHub Actions

2. **Model Deployment**  
   - Docker containerization
   - REST API with FastAPI
   - Deployment to AWS SageMaker or GCP Vertex AI

3. **Advanced Techniques**  
   - AutoML (TPOT, Auto-sklearn)
   - Neural Architecture Search
   - Federated Learning

4. **Monitoring**  
   - Real-time monitoring with Prometheus + Grafana
   - Data drift detection with Evidently
   - Model anomaly detection with Alibi Detect

### Conclusion

All 5 teams performed well on machine learning model development basics. The processes of data preprocessing, model training, and evaluation are solid. However, to reach **production-ready level**, the improvements mentioned above are essential.

Particularly Teams 2, 4, and 5 are complete enough to be immediately deployed in practice with just a bit more refinement. Teams 1 and 3 have strong foundations, so focusing on structuring and modularization will allow them to catch up quickly.

**Key Message**: Creating a "working model" is different from creating a "maintainable system". Starting now, make testing, logging, documentation, and version control habitual. That's the path to becoming a senior engineer.
